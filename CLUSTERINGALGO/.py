# -*- coding: utf-8 -*-
"""COMPANY_PROCESSED_DATA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RHytJhbOgG71JLo6lkAuPupiZz1tmo1a

# COMPANY CLASSIFICATION - **using clustering Algo** #
## ---------START OF DATA PROCESSING------- #
"""

import pandas as pd

df = pd.read_csv('testdata.csv')

df.head()

df.describe()

td = df.columns.str.contains('^Unnamed')

df = df.loc[:,~td]

df.head()

df.describe()

df = df.dropna(subset=['housing_flg'])

df.isnull().head()

col1 = df.iloc[:,0]

col1 = pd.DataFrame(col1)

col1.head()

df = df.rename( columns = {df.columns[1]:"Link"})

from googlesearch import search

#performs a final search for the nan values and replaces in the columns.
#only 36 nan remaining.
count = 0
for val,row in df.iterrows():
        if(str(row[df.columns[1]])==('nan')):
            query = str(row[df.columns[0]])
            print(query)
            for j in search(query,tld = 'com',num = 10, stop =1,pause = 2):
                row[df.columns[1]]= j
                print(j)
                count = count+1
                print(row[df.columns[1]])

col3 = pd.DataFrame()

lst = list()
for val,row in df.iterrows():
    if(str(row[df.columns[1]])=='nan'):
        lst.append(row[df.columns[0]])

col3 = pd.DataFrame(lst)

col3.rename(columns={0:'No LINK Company'}).head()
#col3 holds all nan linked companies

df = df.dropna(subset=[df.columns[1]])
df.describe()
#dropped all nan links with rows

"""## Data is removed of missing links
### *col3* holds the rows with missing links.
### df holds the rows with valid links.

# USING WORDNINJA ON COL3 :3
"""

import wordninja

col3.head()

lst = list()
for val,row in col3.iterrows():
    lst.append(row[col3.columns[0]])

lst2 = list();
for i in lst:
    lst2.append(wordninja.split(i))

lst3 = list()
for i in lst2:
    str = ''
    for j in i:
        str+=j+' '
    lst3.append(str)

altmiss = pd.DataFrame(lst3)

altmiss['link']=' '

for val,row in altmiss.iterrows():
    query = row[altmiss.columns[0]]
    print(query)
    for j in search(query,tld = 'com',num = 10, stop =1,pause = 2):
        row[altmiss.columns[1]]= j
        print(j)
        count = count+1
        print(row[altmiss.columns[1]])

altmiss = altmiss.rename(columns = {altmiss.columns[0]:'Company',altmiss.columns[1]:'Link'})

altmiss.describe()

df = df.rename(columns = {df.columns[0]:'Company',df.columns[1]:'Link'})
df.describe()

mp = df.append(altmiss)
#MP HOLDS FINAL VALUES ALONG WITH ALTERED MISSING VALUES

mp.describe()

"""## FOUND MISSING VALUES - LINK with 100% rate -
### STORED IN MP
"""

duplicateColsDF = mp[mp.duplicated(subset = [mp.columns[1]])] #GETTING ALL DUPLICATES WITH RESPECT TO ONLY COLUMNS
duplicateColsDF.describe()
#value = 591
duplicateRowsDF = mp[mp.duplicated(subset = [mp.columns[0]])] #GETTING ALL DUPLICATES WITH RESPECT TO ONLY ROWS
#value = 27
duplicateDF = mp[mp.duplicated()] #GETTING ALL DUPLICATES WITH RESPECT TO COLUMN AND ROWS
#value = 20

mp2 = mp.drop_duplicates()
mp2.sort_index(axis = 1,inplace = True)

mp2.to_csv(path_or_buf='/home/roshan/Desktop/INTERNSHIP/CLUSTERINGALGO/missing_replaced_rem_dup.csv',columns=['Company','Link'],index=False)

"""# REMOVED DUPLICATE VALUES AND STORED IN MP2 CONVERTED TO csv FORMAT

# -----------------------------------------------------------------------------------------------------------

# Retrieving RELiABLE content
"""

import pandas as pd

newdf = pd.read_csv('missing_replaced_rem_dup.csv')

newdf.head()

newdf.insert(2,'Title',0)

newdf.head()

workdf = newdf.head(50)
workdf.head()

from urllib.parse import urlparse

"""### OBTAINING NAME FROM ACRONYM (LIMITATION: WORKS ONLY FOR THE ACROnYMS that are IFSC code)"""

from googlesearch import search
from bs4 import BeautifulSoup
import requests
import urllib.request

for i,j in workdf.iterrows():
    if(len(workdf['Company'][i]))<=5:
        str_list = list('https://www.ifsccodebank.com/search-by-IFSC-code.aspx?IFSCCode=')
        link2 = workdf['Company'][i]
        print(link2,type(link2))
        str_list.append(link2)
        url = ''.join(str_list)
        print(url)
        response = requests.get(''.join(str_list))
        soup = BeautifulSoup(response.text,'html.parser')
        t = soup.get_text()
        r = t[t.find(link2)+7:]
        workdf['Title'][i] = (r[:r.find('-')-5].strip())
        for j in search(str(workdf['Title'][i]),stop = 1):
            workdf['Link'][i] = j
    else:
        workdf['Title'][i] = 'err'

workdf

"""### FOR non-acronym companies --- OBTAINING TITLE from LINK(column - WEBPAGE) TITLE"""

count = 0
for i,j in workdf.iterrows():
    if workdf['Title'][i]=='err' or workdf['Title'][i].startswith('ifsccode') or workdf['Title'][i]=='0':
        url = workdf['Link'][i]
        print(url)
        try:
            response = requests.get(url,verify = False)
            soup = BeautifulSoup(response.text,"html.parser")
            print(soup.title.text)
            workdf['Title'][i]= soup.title.text
            count = count+1
            print(count)
        except:
            workdf['Title'][i]= '-1'
            continue
print(count)

"""### OBTAINING BANK NAME FROM LINKS THAT HAVE TITLE 'IFSC'"""

for i,j in workdf.iterrows():
    if 'IFSC' in str(workdf['Title'][i]) or workdf['Title'][i]==-1:
        query = workdf['Company'][i] + 'Bank'
        for j in search(query,stop = 1):
            workdf['Link'][i] = j
        workdf['Title'][i] = '0'

workdf

"""### ACCURACY - approx(30/50)"""

work2df = workdf.head(32)

work2df

"""## FOR BETTER DATA -- OBTAINING INFO FROM WIKI.. WHEREVER POSSIBLE"""

work2df['WIKI']=0
for i,j in work2df.iterrows():
    query=work2df['Title'][i] + 'Wikipedia'
    for j in search(query,stop = 1):
        work2df['WIKI'][i] = j
    if('wikipedia' not in str(work2df['WIKI'][i])):
        work2df['WIKI'][i] = work2df['Link'][i]
work2df

"""## OBTAINING CONTENT FROM WIKI ==> or if not available ==> from OFFICIAL LINK"""

work2df['Content'] = 0
for i,j in work2df.iterrows():
    url = work2df['WIKI'][i]
    print(url)
    response = requests.get(url,verify = False)
    soup = BeautifulSoup(response.text,'html.parser')
    t= soup.find('p')
    r = t.find_next_sibling('p')
    if 'NoneType'in str(type(r)):
        work2df['Content'][i] = t.text
    else:
        if r.text.strip() == '':
            y=r.find_next_sibling('p')
            print(y.text)
            work2df['Content'][i] = y.text
        else:
            print(r.text)
            work2df['Content'][i] = r.text

work2df

work2df.to_csv(path_or_buf='/home/roshan/Desktop/INTERNSHIP/CLUSTERINGALGO/.csv',columns=['Company','Link'],index=False)

"""###  Data_Cleaning ==>

#### CREATING FUNCTION because all document must be cleaned (here document is same as content column of DATAFRAME
"""

import pandas as pd

from google.colab import files
uploaded = files.upload()

finaldf = pd.read_csv('finaldf.csv')

def data_clean(content):
    # 1. SPLITTING INTO WORDS
    from nltk.tokenize import word_tokenize
    tokens = word_tokenize(str(content))
    # 2. CONVERT WORDS TO LOWERCASE
    words_punc=list()
    for i in tokens:
        words_punc.append(i.lower())
    # 3. REMOVE PUNCTUATIONS
    import string
    table = str.maketrans('','',string.punctuation)
    alphanum = [i.translate(table) for i in words_punc ]
    # 4. REMOVE NOT CHAR TOKENS
    alphaonly = [i for i in alphanum if i.isalpha() ]
    # 5. REMOVE STOPWORDS
    from nltk.corpus import stopwords
    stop_words = set(stopwords.words('english'))
    words = [i for i in alphaonly if not i in stop_words]

    ### Lemmatizing OF WORDS
    #!!!!! ---- NOT STEMMING BECAUSE IT CUTS SHORT THE IMPORTANT WORDS -----!!!!#
    # NEED TO TRY WITH STEMMING #

    from nltk.stem import WordNetLemmatizer
    lemmatizer = WordNetLemmatizer()

    word = list()
    for i in words:
        word.append(lemmatizer.lemmatize(i,'v'))

    return(word)

"""#### TESTDATA"""

def dummy_fun(doc):
    return doc

lst = list()
for i in range(len(finaldf['Content'])):
    lst.append(data_clean(finaldf['Content'][i]))

import nltk
nltk.download('wordnet')

len(lst)



"""# Processing Textual Data using TF-IDF"""

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(use_idf = True,sublinear_tf=True,analyzer='word',preprocessor=dummy_fun,tokenizer=dummy_fun,norm='l2')

vec_1 = vectorizer.fit_transform(lst)
type(vec_1)

len(vectorizer.get_feature_names())

vec_1.shape

vect_df = pd.DataFrame(vec_1.toarray(),columns = vectorizer.get_feature_names())

for i in vect_df.columns:
  count = 0
  for j,k in vect_df.iterrows():
    if(vect_df[i][j])!=0:
      count = count+1
  if(count ==1):
    del vect_df[i]

vect_df.shape

"""# ----------------------------------------------------------------------------------------------------------
## Is Document Similiraity needed?
### Each company need not be cross checked with each other for similarity. Although, there could be a possibility that algorithm may improve if similar documents are clubbed together into a corresponding segment.

# Need to Test Doc Similarity
# ----------------------------------------------------------------------------------------------------------

# Performing Clustering..
"""

from sklearn.cluster import KMeans

true_k = 3
kmeans = KMeans(n_clusters = true_k,init = 'k-means++',max_iter = 500,n_init = 1)
kmeans = kmeans.fit(vect_df)
kmeans

clusters = kmeans.predict(vect_df)
pca = PCA(n_components=2)
two_dim = pca.fit_transform(vect_df.to_dense())

scatter_x = two_dim[:,0]
scatter_y = two_dim[:,1]

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

plt.style.use('ggplot')
fig,ax = plt.subplots()
fig.set_size_inches(20,10)
cmap = {0:'green',1:'red',2:'yellow'}
for group in np.unique(clusters):
  ix = np.where(clusters == group)
  ax.scatter(scatter_x[ix], scatter_y[ix], c=cmap[group], label=group)
ax.legend()
plt.xlabel('pCA 0')
plt.ylabel('pca 1')
plt.show()

for i in range (true_k):
  print('Cluster %d: '%i)
  for ind in order_centroids[i,:10]:
    print('%s' % terms[ind])

