{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import Comp_classify_def as apply\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer, RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roshan/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from googlesearch import search\n",
    "import wordninja\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Getting the labeled companies\n",
    "workdf = pd.read_csv('testdata_Labeled.csv')\n",
    "workdf.drop([workdf.columns[1],'Unnamed: 3','Unnamed: 4','Unnamed: 5','Unnamed: 6',\n",
    "            'Unnamed: 7'],axis = 1,inplace = True)\n",
    "workdf.drop(0,axis = 0,inplace = True)\n",
    "workdf = workdf.head(73)\n",
    "#Removing the Categories that are Bank to provide independency.\n",
    "workdf.drop(workdf[workdf['Category']=='Bank'].index,axis = 0,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Limited Data but works for now\n",
    "workdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "workdf = apply.transform_data(workdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def link_obtain(X_df):\n",
    "    for i,j in X_df.iterrows():\n",
    "        URL = X_df['housing_flg'][i]\n",
    "        query = URL\n",
    "        print(query)\n",
    "        #Directly Retrieving all ifsc code data without saving link in column.\n",
    "        if(len(str(query.strip())))<5:\n",
    "            t =0\n",
    "            str_list = list('https://www.ifsccodebank.com/search-by-IFSC-code.aspx?IFSCCode=')\n",
    "            str_list.append(query)\n",
    "            url = ''.join(str_list)\n",
    "            response = requests.get(''.join(str_list))\n",
    "            soup = BeautifulSoup(response.text,'html.parser')\n",
    "            t = soup.get_text()\n",
    "            r = t[t.find(query)+7:]\n",
    "            r = (r[:r.find('-')-5].strip())\n",
    "            X_df['Company'][i] = r\n",
    "            q2 = str(X_df['Company'][i])+' wiki'\n",
    "            print(q2)\n",
    "            if 'com' not in q2:\n",
    "                t =0\n",
    "                for l in search(q2,stop = 2):\n",
    "                    stri = 'Link'+str(t)\n",
    "                    print(q2,l)\n",
    "                    X_df[stri][i] = l\n",
    "                    t = int(t)+1\n",
    "                    print(stri)\n",
    "            else:\n",
    "                X_df['Company'][i] = 'Nan'\n",
    "            continue\n",
    "        #if Name has no space ---> Using wordninja algo to split the words.\n",
    "        elif ' ' not in query.strip():\n",
    "            query = \" \".join(str(x) for x in wordninja.split(query))+' Wiki'\n",
    "    ####===> The following gets the link and not the data <===####    \n",
    "        # Direct search for appropriate official Link    \n",
    "        try:\n",
    "            t = 0\n",
    "            for k in search(query,stop = 2):\n",
    "                stri = 'Link' + str(t)\n",
    "                X_df[stri][i] = k\n",
    "                t = t+1\n",
    "                print(stri)\n",
    "        # Search the words with key word Financial Services\n",
    "            t = 2\n",
    "            query = URL + ' Finanancial Services' + ' Wiki'\n",
    "            for k in search(query,stop = 2):\n",
    "                stri = 'Link' + str(t)\n",
    "                X_df[stri][i] = k \n",
    "                t = t+1\n",
    "                print(stri)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            # except block to store the company which has bad request/other issue\n",
    "            X_df['Link4'][i] = query\n",
    "            print('Errored out: ',query)\n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AADHAR HOUSING FINAN\n",
      "Link0\n",
      "Link1\n",
      "Link2\n",
      "Link3\n",
      "AAMAADMIPARTY\n",
      "Link0\n",
      "Link1\n",
      "Link2\n",
      "Link3\n",
      "Aavas Financiers Lim\n",
      "Link0\n",
      "Link1\n",
      "Link2\n",
      "Link3\n",
      "ABHIPRA CAPITAL LTD\n",
      "Link0\n",
      "Link1\n",
      "Link2\n",
      "Link3\n",
      "DR FILE DTD \n",
      "Link0\n",
      "Link1\n",
      "Link2\n",
      "Link3\n",
      "DR TPZ DT \n",
      "Link0\n",
      "Link1\n",
      "Link2\n",
      "Link3\n",
      "ACHAL\n",
      "Link0\n",
      "Link1\n"
     ]
    }
   ],
   "source": [
    "workdf = link_obtain(workdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "workdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    soup = BeautifulSoup(text, \"lxml\")\n",
    "    tags_del = soup.get_text()\n",
    "    no_html = re.sub('<[^>]*>', '', tags_del)\n",
    "    tokenized = casual_tokenizer(no_html)\n",
    "    lower = [item.lower() for item in tokenized]\n",
    "    tagged = nltk.pos_tag(lower)\n",
    "    lemma = lemma_wordnet(tagged)\n",
    "    no_num = [re.sub('[0-9]+', '', each) for each in lemma]\n",
    "    no_punc = [w for w in no_num if w not in punc]\n",
    "    no_stop = [w for w in no_punc if w not in stop_words]\n",
    "    return no_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to replace the nltk pos tags with the corresponding wordnet pos tag to use the wordnet lemmatizer\n",
    "def get_word_net_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def lemma_wordnet(tagged_text):\n",
    "    final = []\n",
    "    for word, tag in tagged_text:\n",
    "        wordnet_tag = get_word_net_pos(tag)\n",
    "        if wordnet_tag is None:\n",
    "            final.append(lemmatizer.lemmatize(word))\n",
    "        else:\n",
    "            final.append(lemmatizer.lemmatize(word, pos=wordnet_tag))\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_desc(X_df):\n",
    "    for i,j in X_df.iterrows():\n",
    "            if 'Nan' not in X_df['Link0'][i]:\n",
    "                response = requests.get(X_df['Link0'][i],verify = False)\n",
    "                t = process_text(response.text)\n",
    "                X_df['Data0'][i] = t\n",
    "            if 'Nan' not in X_df['Link1'][i]:\n",
    "                response = requests.get(X_df['Link1'][i],verify = False)\n",
    "                t = process_text(response.text)\n",
    "                X_df['Data1'][i] = t\n",
    "            if 'Nan' not in X_df['Link2'][i]:\n",
    "                response = requests.get(X_df['Link2'][i],verify = False)\n",
    "                t = process_text(response.text)\n",
    "                X_df['Data2'][i] = t\n",
    "            if 'Nan' not in X_df['Link3'][i]:\n",
    "                response = requests.get(X_df['Link3'][i],verify = False)\n",
    "                t = process_text(response.text)\n",
    "                X_df['Data3'][i] = t\n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TO show IMBALANCED CLASSES:\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "workdf.groupby('Category').housing_flg.count().plot.bar(ylim = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def casual_tokenizer(text): #Splits words on white spaces (leaves contractions intact) and splits out trailing punctuation\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roshan/.local/lib/python3.5/site-packages/urllib3/connectionpool.py:851: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-3010afcdf78d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_desc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-2c642b118092>\u001b[0m in \u001b[0;36mdata_desc\u001b[0;34m(X_df)\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'Nan'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Link0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Link0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverify\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                 \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m                 \u001b[0mX_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Data0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'Nan'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Link1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-02f8e6ebc17a>\u001b[0m in \u001b[0;36mprocess_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtags_del\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mno_html\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<[^>]*>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags_del\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcasual_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_html\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mlower\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdecontract\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mexpandContractions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_re\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc_re\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-65f8e3892c20>\u001b[0m in \u001b[0;36mcasual_tokenizer\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcasual_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#Splits words on white spaces (leaves contractions intact) and splits out trailing punctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "data_desc(workdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
