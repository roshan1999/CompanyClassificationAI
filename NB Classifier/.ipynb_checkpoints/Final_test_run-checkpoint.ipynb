{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter file with labeled data or press 2 to skip train: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roshan/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:155: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/roshan/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:156: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 0 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5db412c33bba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'testdata_Labeled.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m         \u001b[0mdata_processed_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrans_data_to_vect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0mcomp_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Enter company name to classify: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-5db412c33bba>\u001b[0m in \u001b[0;36mprocess_data\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mdata_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0mdata_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m     \u001b[0mdata_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhousing_flg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/roshan/anaconda3/lib/python3.5/site-packages/pandas/indexes/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for axis 0 with size 3"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from googlesearch import search\n",
    "import wordninja\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import Comp_classify_def\n",
    "\"\"\"\n",
    "This python file stores all definitions used in Comp_Classify_final.py\n",
    "\"\"\"\n",
    "def transform_data (inputseries):\n",
    "    X_df = pd.DataFrame(inputseries)\n",
    "    X_df['Company'] = 'Nan'\n",
    "    for i in range(5):\n",
    "        stri = 'Link'+str(i)\n",
    "        X_df[stri] = 'Nan'\n",
    "        stri = 'Data'+str(i)\n",
    "        X_df[stri] = 'Nan'\n",
    "    return X_df\n",
    "#returns X_df (Dataframe having added columns.)\n",
    "def cleanme(html):\n",
    "    soup = BeautifulSoup(html) # create a new bs4 object from the html data loaded\n",
    "    for script in soup([\"script\"]):\n",
    "        script.extract()\n",
    "    text = soup.get_text()\n",
    "    text = text.replace('\\n', ' ').replace('\\r', '').replace('\\t',' ')\n",
    "    return text.strip()\n",
    "#cleans the response from website.\n",
    "def get_df_with_lmtd_desc(X):\n",
    "    X_df = link_obtain(X)\n",
    "    X_df = lmtd_data_desc(X_df)\n",
    "    return X_df\n",
    "#combines transform_data, link_obtain and lmtd_data_desc functions\n",
    "def get_df_with_desc(X):\n",
    "    X_df = link_obtain(X)\n",
    "    X_df = data_desc(X_df)\n",
    "    return X_df\n",
    "#combines transform_data, link_obtain and data_desc functions\n",
    "def link_obtain(X_train):\n",
    "    count = 0\n",
    "    X_df = transform_data(X_train)\n",
    "    for i,j in X_df.iterrows():\n",
    "        URL = X_df['housing_flg'][i]\n",
    "        query = URL\n",
    "        #print(query)\n",
    "        #Directly Retrieving all ifsc code data without saving link in column.\n",
    "        if(len(str(query.strip())))<5:\n",
    "            t =0\n",
    "            str_list = list('https://www.ifsccodebank.com/search-by-IFSC-code.aspx?IFSCCode=')\n",
    "            str_list.append(query)\n",
    "            url = ''.join(str_list)\n",
    "            response = requests.get(''.join(str_list))\n",
    "            soup = BeautifulSoup(response.text,'html.parser')\n",
    "            t = soup.get_text()\n",
    "            r = t[t.find(query)+7:]\n",
    "            r = (r[:r.find('-')-5].strip())\n",
    "            X_df['Company'][i] = r\n",
    "            q2 = str(X_df['Company'][i])\n",
    "            if 'com' not in q2:\n",
    "                t =0\n",
    "                for l in search(q2,stop = 2):\n",
    "                    stri = 'Link'+str(t)\n",
    "                    #print(q2,l)\n",
    "                    X_df[stri][i] = l\n",
    "                    count = count+1\n",
    "                    t = int(t)+1\n",
    "                    #print(stri)\n",
    "            else:\n",
    "                X_df['Company'][i] = 'Nan'\n",
    "            continue\n",
    "        #if Name has no space ---> Using wordninja algo to split the words.\n",
    "        elif ' ' not in query.strip():\n",
    "            query = \" \".join(str(x) for x in wordninja.split(query))\n",
    "    ####===> The following gets the link and not the data <===####\n",
    "        # Direct search for appropriate official Link\n",
    "        try:\n",
    "            t = 0\n",
    "            for k in search(query,stop = 2):\n",
    "                stri = 'Link' + str(t)\n",
    "                X_df[stri][i] = k\n",
    "                t = t+1\n",
    "                #print(stri)\n",
    "        # Search the words with key word Financial Services\n",
    "            t = 2\n",
    "            query = URL + ' Finanancial Services'\n",
    "            for k in search(query,stop = 2):\n",
    "                stri = 'Link' + str(t)\n",
    "                X_df[stri][i] = k\n",
    "                t = t+1\n",
    "                #print(stri)\n",
    "            count = count+1\n",
    "        except:\n",
    "            # except block to store the company which has bad request/other issue\n",
    "            X_df['Link4'][i] = query\n",
    "            print('Errored out: ',query)\n",
    "        print('Completed : ')\n",
    "        print(count//15,end = '\\r')\n",
    "    return X_df\n",
    "#takes series and returns dataframe with link obtained from the respective companies.\n",
    "def lmtd_data_desc(X_df):\n",
    "    for i,j in X_df.iterrows():\n",
    "        if 'Nan' not in X_df['Link0'][i]:\n",
    "            response = requests.get(X_df['Link0'][i],verify = False)\n",
    "            t = cleanme(response.text)\n",
    "            X_df['Data0'][i] = t\n",
    "    return X_df\n",
    "#returns dataframe with data description obtained from first link\n",
    "def data_desc(X_df):\n",
    "    for i,j in X_df.iterrows():\n",
    "        try:\n",
    "            if 'Nan' not in X_df['Link3'][i]:\n",
    "                response = requests.get(X_df['Link3'][i],verify = False)\n",
    "                t = cleanme(response.text)\n",
    "                X_df['Data3'][i] = t\n",
    "            if 'Nan' not in X_df['Link0'][i]:\n",
    "                response = requests.get(X_df['Link0'][i],verify = False)\n",
    "                t = cleanme(response.text)\n",
    "                X_df['Data0'][i] = t\n",
    "        except:\n",
    "            print('Error: ',i)\n",
    "    return X_df\n",
    "#returns dataframe with data description obtained from first and third link.\n",
    "def transform_to_vector(vectorizer,X_df):\n",
    "    X_res = pd.DataFrame(columns = vectorizer.get_feature_names())\n",
    "    for i,j in X_df.iterrows():\n",
    "        print(X_df['housing_flg'][i])\n",
    "        rtlst = [0]*len(vectorizer.get_feature_names())\n",
    "        rt = vectorizer.transform([X_df['Data0'][i]]).toarray()\n",
    "        for k in range(rt.shape[1]):\n",
    "            rtlst[k] = rt[0][k]+rtlst[k]\n",
    "            print ('Initial ',i, rtlst[k])\n",
    "        rt = vectorizer.transform([X_df['Data3'][i]]).toarray()\n",
    "        for k in range(rt.shape[1]):\n",
    "            rtlst[k] = rt[0][k]+rtlst[k]\n",
    "            print('Final ',i,rtlst[k])\n",
    "        X_res.loc[i] = rtlst\n",
    "    return X_res\n",
    "#vectorizes the dataframe text into numbers => returns dataframe with vectorized\n",
    "#text according to the features.\n",
    "\n",
    "\n",
    "def process_data(filename):\n",
    "    \"\"\"\n",
    "    Processing DataSet\n",
    "    \"\"\"\n",
    "    #Getting Data from testdata_Labeled.csv file.\n",
    "    data = pd.read_csv(filename)\n",
    "    #Retrieving only data which are labeled for now\n",
    "    data_df = data.head(15)\n",
    "    #Dropping columns with missing value\n",
    "    data_df.dropna(how='all',axis=1,inplace=True)\n",
    "    data_df.dropna(how='all',axis=0,inplace=True)\n",
    "    #data_df.drop(axis=1,labels=[data_df.columns[3],data_df.columns[4]],inplace=True)\n",
    "    X = data_df.housing_flg\n",
    "    y = data_df.Category\n",
    "    return X,y\n",
    "    # \"\"\"\n",
    "    # Splitting data into train and test sets among the category before applying\n",
    "    # vectorization\n",
    "    # \"\"\"\n",
    "    # #X_train,X_test,Y_train,Y_test = train_test_split(X,y,random_state = 1)\n",
    "    #\n",
    "def trans_data_to_vect(X):\n",
    "    \"\"\"\n",
    "    get_df_with_desc transforms the series into a dataframe having\n",
    "    columns filled with respective values\n",
    "    \"\"\"\n",
    "    X_df = get_df_with_lmtd_desc(X)\n",
    "\n",
    "    \"\"\"\n",
    "    Vectorizing data with TfidfVectorizer\n",
    "    \"\"\"\n",
    "    #configuring TFidf Vectorizer with predefined features.\n",
    "    vectorizer = TfidfVectorizer(use_idf = 'False',binary=True,ngram_range=(1,2))\n",
    "    vectorizer.fit(['Finance', 'Loan', 'Insurance',\n",
    "    'Investor' ,'Business Services','Financial Services', 'Personal Loan', 'Mobile Loan',\n",
    "    'Consumer Durable Loan', 'Furniture Loan',\n",
    "    '2-wheeler Loan', 'E - Bike Loan', 'Bank', 'Home Loan', 'Housing Finance'])\n",
    "    vectorizer.get_feature_names()\n",
    "    X_res = transform_to_vector(vectorizer,X_df)\n",
    "    #X_res is a dataframe that stores the vectorized data\n",
    "    return X_res\n",
    "def train_test(data_df,num,comp_name):\n",
    "    \"\"\"\n",
    "    Splitting data into train and test sets among the category before applying\n",
    "    vectorization\n",
    "    \"\"\"\n",
    "    X_df = data_df.head(num)\n",
    "\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(X_df,y,random_state = 1)\n",
    "\n",
    "    #Trying MultinomialNB and KNN classifier algos:\n",
    "\n",
    "    categories = ['Finance','Assmt','Investor','Loan','Bank','Home Loan','other']\n",
    "    print('Classification by MultinomialNB: ')\n",
    "    alp = 0\n",
    "    max_scr = 0\n",
    "    for k in range(5):\n",
    "        model = MultinomialNB(alpha = k)\n",
    "        model.fit(X_train,Y_train)\n",
    "        labels = model.predict(X_test)\n",
    "        scr = accuracy_score(labels,Y_test)\n",
    "        if scr >max_scr:\n",
    "            max_scr = scr\n",
    "            alpha = k\n",
    "        print('score for alpha ', k,' is ' , scr)\n",
    "    n = 1\n",
    "    max_scr = 0\n",
    "    print('Classification by K nearest Neighbor: ')\n",
    "    for k in range(25):\n",
    "        modelknn = KNeighborsClassifier(n_neighbors=(k+1))\n",
    "        modelknn.fit(X_train.drop(X_train,Y_train))\n",
    "        labelknn = modelknn.predict(X_test)\n",
    "        scr = accuracy_score(Y_test,labelknn)\n",
    "        print('Accuracy for ',k+1,' neighbors = ',scr)\n",
    "        if scr>max_scr:\n",
    "            max_scr = scr\n",
    "            n = k\n",
    "    comp_name_x = pd.DataFrame([comp_name],columns = {'housing_flg'})\n",
    "    comp_df = trans_data_to_vect(comp_name_x)\n",
    "    model = MultinomialNB(alpha =alp)\n",
    "    model.fit(X_train,Y_train)\n",
    "    labels = model.predict(comp_df)\n",
    "    print('Multinomial NB  label: ',labels)\n",
    "    modelknn = KNeighborsClassifier(n_neighbors=n+1)\n",
    "    modelknn.fit(X_train.drop(X_train,Y_train))\n",
    "    labels = modelknn.predict(comp_df)\n",
    "    print('Multinomial NB  label: ',labels)\n",
    "if __name__ == '__main__':\n",
    "    filename = input('Enter file with labeled data or press 2 to skip train: ')\n",
    "    if filename!='2':\n",
    "        X,y = process_data(filename)\n",
    "        data_processed_df = trans_data_to_vect(X)\n",
    "        num = input('Enter the number of data already labeled: ')\n",
    "        comp_name = input('Enter company name to classify: ')\n",
    "        train_test(data_processed_df,y,74,comp_name)\n",
    "    else:\n",
    "        filename = 'testdata_Labeled.csv'\n",
    "        X,y = process_data(filename)\n",
    "        data_processed_df = trans_data_to_vect(X)\n",
    "        comp_name = input('Enter company name to classify: ')\n",
    "        train_test(data_processed_df,y,74,comp_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
